1.  anaconda的问题(?)：安装tensorflow在虚拟环境中，在ide中的终端启动了虚拟环境，并在此终端中成功打印了tensor的版本号，但是在运行文件时却有“无法解析导入tensorflow”的报错，而chatgpt说ide有对虚拟环境的支持并且应当会自动切换python解释器，在ide的终端中启动应当属于这种状况。没搞明白。现在直接在虚拟机系统环境中安装了tf，但是在终端中执行的打印版本号程序时却出现了no module named tensorflow问题,是的，情况反转了，在命令行中报错，在ide的一键执行却能成功（当然有2中的报错），我不确定这种关系是否有什么张力
2.  tensorflow的问题：出现cuDNN cuFFT cuBLAS 的Unable to register xxx factory 错误，并且在github上找到了最近的issue：https://github.com/tensorflow/tensorflow/issues/62075（仍然是open状态），说明此问题不是个例，并且cuDNN和cuda与tensorflow之间的包含关系实在令人感到混乱（我分别用包管理器安装了cuda-toolkit和tensorflow，并被告知的是cuDNN已经被包含在tf或者cuda里，而我看到的安装可选项中还有tensor-gpu和tensor[and cuda]）
    在出现这些报错的时候我依然调用函数并成功打印出版本号，但是如果试图去运行MNIST的训练代码就会出现其他的报错无法完成：TF-TRT Warning: Could not find TensorRT，按照chatgpt的说法可能是需要特定版本的tf（我第一次在虚拟环境中尝试的是1.2,在系统环境中分别尝试过1.2和2.14），否则可能需要自己下载TRT并用Bazel重新构建（大概是说编译？）tf（包括还有一条information级别的信息说机器支持xx指令集，请重构tf来应用），这超出了我目前要关心的范围（其实说起来感觉大体就是那些框架和特定库之间的支持关系问题，TRT是跟显卡有关的？也许有一个“切换到纯cpu模式”可以暂时先把燃眉之急解了）
3.  虚拟机的问题：tf中的函数结果表明我没有可用的gpu，虽然vmware应该默认有显卡虚拟化，但是也许没法被cuda使用？
    目前查找到的显卡设备只有一个VGA compatible controller，也许我的geforce显卡没有被虚拟化进来？没搞明白。日后直接使用双系统应该可以满足学习要求。不过这比起2来说目前不是什么值得关注的问题。


4.  此时已经来到了10月23号的晚上9点，我决定暂时放弃一切对环境配置的理解的尝试，试着用docker来进行。
    下载了一个tensorflow的image，尝试在desktop中启动，点击启动按钮没反应，status一直是exit（0）
    尝试用那个菜单栏里的dev env，在那个容器的下方新建了一个开发环境，在vscode里直接打开了，如果能用vscode，确实之前预期的只能用shell就显得很麻烦。但是这原来是一个完全空的开发环境，完全没有任何东西，不是我想要的。
    启动失败是没打开hyper-v？docker对这个竟然也完全没有提示。->打开hyper-v后也还是一样的结果。
    试图在命令行中操作，试图给run加上--privileged标签，但是都没有用。
    我参照一个教程给run命令加上-t标签，成功运行了docker容器。（docker容器一旦没有前台进程就会退出并返回0，-t为分配一个伪终端给这个容器，即一个长期运行的进程）现在容器里有一个bash，将py文件传到容器里，我可以用vim编辑并解释运行。


5.  现在终于来到了代码测试阶段。我将在docker的命令行界面中运行使用tf的教程bp网络代码，然后依然在vmware中尝试理解和运行用pytorch构建的cnn网络。
    我在容器中运行了教程的py文件，没有输出，只有一个目前无关紧要的I级信息，跟之前在vmware里见过的一样，不过没有报错。但z是输出呢？
    原来是它代码的运行部分跟类的定义分开了，原作者要求用独立的运行代码来调用这里定义的库。我把它们合并到一起。
    我没法找到教程使用的mnist_loader包，我把mnist的导入方式改为使用python-mnist包->我不确定MNIST包的使用方法，改用tf自带的mnist
    教程的代码并不能直接运行。